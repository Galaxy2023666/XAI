{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Galaxy2023666/XAI/blob/main/5G_NIDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPolDyItP5Z3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, confusion_matrix, classification_report,\n",
        "                             ConfusionMatrixDisplay)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# !pip install tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from xgboost import XGBClassifier, XGBRegressor, plot_importance, plot_tree\n",
        "\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zLffwZ02LwiI"
      },
      "outputs": [],
      "source": [
        "# 1. read CSV\n",
        "df = pd.read_csv('/content/drive/MyDrive/XAI_DATA/Combined.csv',encoding='utf-8')\n",
        "\n",
        "# 2. show the sample data\n",
        "print(\"data sample：\")\n",
        "print(df.head())\n",
        "\n",
        "# 3. basic info\n",
        "print(\"\\nbasic info：\")\n",
        "print(df.describe(include='all'))\n",
        "\n",
        "# handle NaN\n",
        "print(df.info())\n",
        "MIN_NON_NULL=1215000\n",
        "df_cleaned = df.dropna(axis=1, thresh=MIN_NON_NULL)\n",
        "print(df_cleaned.info())\n",
        "\n",
        "# object\n",
        "object_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Object（{len(object_cols)}）：\\n{object_cols}\")\n",
        "\n",
        "analysis_report = pd.DataFrame({\n",
        "        'column': object_cols,\n",
        "        'unique': [df[col].nunique() for col in object_cols],\n",
        "        'Nan': [df[col].isnull().sum() for col in object_cols],\n",
        "        'Nan rate': [df[col].isnull().mean().round(4) for col in object_cols],\n",
        "        'frequency': [df[col].mode()[0] if not df[col].empty else np.nan for col in object_cols],\n",
        "        'frequency rate': [round(df[col].value_counts(normalize=True).iloc[0], 4) for col in object_cols],\n",
        "        'sample': [df[col].dropna().iloc[0] if not df[col].dropna().empty else np.nan for col in object_cols]\n",
        "})\n",
        "\n",
        "print(\"\\nObject：\")\n",
        "display(analysis_report.sort_values('unique', ascending=False))\n",
        "\n",
        "for col in object_cols:\n",
        "    value_counts = df[col].value_counts(dropna=False)\n",
        "    unique_count = len(value_counts)\n",
        "\n",
        "    print(f\"\\n=== column【{col}】 ===\")\n",
        "    print(f\"unique{unique_count}\")\n",
        "    print(value_counts.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rYw5F4Y9vL5P"
      },
      "outputs": [],
      "source": [
        "# data processing\n",
        "# merge some categories\n",
        "df_cleaned['Proto'] = np.where(df_cleaned['Proto'].isin(['lldp','llc','arp','ipv6-icmp']), 'other', df_cleaned['Proto'])\n",
        "df_cleaned['State'] = np.where(df_cleaned['State'].isin(['ECO','ACC','URP','RSP','TST','NRS']), 'other', df_cleaned['State'])\n",
        "\n",
        "# # encode\n",
        "# proto_freq = df['Proto'].value_counts(normalize=True)\n",
        "# df['Proto_encoded'] = df['Proto'].map(proto_freq)\n",
        "proto_dummies = pd.get_dummies(df_cleaned['Proto'], prefix='Proto')\n",
        "cause_dummies = pd.get_dummies(df_cleaned['Cause'], prefix='Cause')\n",
        "state_dummies = pd.get_dummies(df_cleaned['State'], prefix='State')\n",
        "df_new = pd.concat([df_cleaned, proto_dummies, cause_dummies,state_dummies], axis=1)\n",
        "df_new.drop(['Proto', 'Cause','State','sDSb'], axis=1, inplace=True)\n",
        "df_new=df_new.dropna(axis=0)\n",
        "print(df_new.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV82xhDR7npQ"
      },
      "outputs": [],
      "source": [
        "features=df_new.columns.tolist()\n",
        "features.remove('Label')\n",
        "features.remove('Attack Type')\n",
        "features.remove('Attack Tool')\n",
        "features.remove('Unnamed: 0')\n",
        "print(features)\n",
        "X = df_new[features]\n",
        "le = LabelEncoder()\n",
        "y = df_new['Label']\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdcIMd8G1PwF"
      },
      "outputs": [],
      "source": [
        "label_counts = df_new['Label'].value_counts()\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM525t-qoBi5"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Union\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class XAIAnalyzer:\n",
        "    \"\"\"\n",
        "    XAI可解释性分析器，集成SHAP和LIME功能\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, X_train: pd.DataFrame, class_names: List[str] = None):\n",
        "        \"\"\"\n",
        "        初始化XAI分析器\n",
        "\n",
        "        Args:\n",
        "            model: 训练好的机器学习模型\n",
        "            X_train: 训练数据特征\n",
        "            class_names: 类别名称列表\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.X_train = X_train\n",
        "        self.class_names = class_names or [f'Class_{i}' for i in range(2)]\n",
        "        self.explainer = None\n",
        "        self.lime_explainer = None\n",
        "\n",
        "    def _create_shap_explainer(self, sample_size: int = 100):\n",
        "        \"\"\"创建SHAP解释器\"\"\"\n",
        "        if self.explainer is None:\n",
        "            sample_data = shap.sample(self.X_train, min(sample_size, len(self.X_train)))\n",
        "            self.explainer = shap.KernelExplainer(self.model.predict, sample_data)\n",
        "        return self.explainer\n",
        "\n",
        "    def _create_lime_explainer(self):\n",
        "        \"\"\"创建LIME解释器\"\"\"\n",
        "        if self.lime_explainer is None:\n",
        "            self.lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "                training_data=self.X_train.values,\n",
        "                feature_names=self.X_train.columns.tolist(),\n",
        "                class_names=self.class_names,\n",
        "                mode='classification'\n",
        "            )\n",
        "        return self.lime_explainer\n",
        "\n",
        "    def generate_shap_summary_plots(self, X_test: pd.DataFrame, max_samples: int = 1000):\n",
        "        \"\"\"\n",
        "        生成SHAP总体分析图（bar plot和beeswarm plot）\n",
        "\n",
        "        Args:\n",
        "            X_test: 测试数据\n",
        "            max_samples: 最大样本数量\n",
        "        \"\"\"\n",
        "        print(\"正在生成SHAP总体分析图...\")\n",
        "\n",
        "        # 创建解释器\n",
        "        explainer = self._create_shap_explainer()\n",
        "\n",
        "        # 限制样本数量以提高性能\n",
        "        test_sample = X_test.iloc[:min(max_samples, len(X_test))]\n",
        "\n",
        "        # 计算SHAP值\n",
        "        print(f\"正在计算 {len(test_sample)} 个样本的SHAP值...\")\n",
        "        shap_values = explainer.shap_values(test_sample)\n",
        "\n",
        "        # 生成summary plot - bar plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(shap_values, test_sample, plot_type=\"bar\", show=False)\n",
        "        plt.title(\"SHAP Feature Importance (Bar Plot)\", fontsize=14, pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 生成summary plot - beeswarm plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(shap_values, test_sample, show=False)\n",
        "        plt.title(\"SHAP Feature Effects (Beeswarm Plot)\", fontsize=14, pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return shap_values\n",
        "\n",
        "    def find_wrong_predictions(self, X_test: pd.DataFrame, y_test: pd.Series) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        找出模型的错误预测\n",
        "\n",
        "        Args:\n",
        "            X_test: 测试特征\n",
        "            y_test: 测试标签\n",
        "\n",
        "        Returns:\n",
        "            wrong_indices: 错误预测的索引\n",
        "            y_pred: 所有预测结果\n",
        "        \"\"\"\n",
        "        print(\"正在分析模型预测结果...\")\n",
        "\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        wrong_indices = np.where(y_pred != y_test)[0]\n",
        "\n",
        "        print(f\"总样本数: {len(y_test)}\")\n",
        "        print(f\"错误预测数: {len(wrong_indices)}\")\n",
        "        print(f\"错误率: {len(wrong_indices)/len(y_test):.2%}\")\n",
        "\n",
        "        if len(wrong_indices) > 0:\n",
        "            print(f\"错误预测样本索引: {wrong_indices[:10]}...\" if len(wrong_indices) > 10 else f\"错误预测样本索引: {wrong_indices}\")\n",
        "\n",
        "        return wrong_indices, y_pred\n",
        "\n",
        "    def analyze_wrong_predictions_shap(self, X_test: pd.DataFrame, y_test: pd.Series,\n",
        "                                     max_samples: int = 5, class_index: Optional[int] = None):\n",
        "        \"\"\"\n",
        "        使用SHAP分析错误预测样本\n",
        "\n",
        "        Args:\n",
        "            X_test: 测试特征\n",
        "            y_test: 测试标签\n",
        "            max_samples: 分析的最大样本数\n",
        "            class_index: 指定分析的类别索引（多分类时使用）\n",
        "        \"\"\"\n",
        "        wrong_indices, y_pred = self.find_wrong_predictions(X_test, y_test)\n",
        "\n",
        "        if len(wrong_indices) == 0:\n",
        "            print(\"🎉 模型没有错误预测！\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n📊 正在分析前 {min(max_samples, len(wrong_indices))} 个错误预测样本的SHAP解释...\")\n",
        "\n",
        "        # 创建解释器\n",
        "        explainer = self._create_shap_explainer()\n",
        "\n",
        "        # 选择要分析的样本\n",
        "        sample_indices = wrong_indices[:max_samples] if len(wrong_indices) > max_samples else wrong_indices\n",
        "        X_wrong = X_test.iloc[sample_indices]\n",
        "\n",
        "        # 计算SHAP值\n",
        "        shap_values = explainer.shap_values(X_wrong)\n",
        "\n",
        "        # 为每个错误样本生成waterfall图\n",
        "        for i, idx in enumerate(sample_indices):\n",
        "            self._plot_single_shap_waterfall(explainer, shap_values, X_test, y_test,\n",
        "                                           y_pred, idx, i, class_index)\n",
        "\n",
        "    def _plot_single_shap_waterfall(self, explainer, shap_values, X_test, y_test,\n",
        "                                  y_pred, idx, i, class_index):\n",
        "        \"\"\"绘制单个样本的SHAP waterfall图\"\"\"\n",
        "        print(f\"\\n样本 {idx}:\")\n",
        "        print(f\"真实标签: {y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]} ({self.class_names[y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]]})\")\n",
        "        print(f\"预测标签: {y_pred[idx]} ({self.class_names[y_pred[idx]]})\")\n",
        "\n",
        "        # 处理多分类和二分类情况\n",
        "        if isinstance(shap_values, list):\n",
        "            # 多分类模型\n",
        "            target_class = class_index if class_index is not None else y_pred[idx]\n",
        "            values = shap_values[target_class][i]\n",
        "            base_value = explainer.expected_value[target_class]\n",
        "            plot_title = f\"样本 {idx} - {self.class_names[target_class]} 类别的SHAP解释\"\n",
        "        else:\n",
        "            # 二分类模型\n",
        "            values = shap_values[i]\n",
        "            base_value = explainer.expected_value\n",
        "            plot_title = f\"样本 {idx} - SHAP解释\"\n",
        "\n",
        "        # 创建解释对象\n",
        "        explanation = shap.Explanation(\n",
        "            values=values,\n",
        "            base_values=base_value,\n",
        "            data=X_test.iloc[idx],\n",
        "            feature_names=X_test.columns.tolist()\n",
        "        )\n",
        "\n",
        "        # 绘制waterfall图\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.waterfall_plot(explanation, show=False)\n",
        "        plt.title(plot_title, fontsize=14, pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def analyze_wrong_predictions_lime(self, X_test: pd.DataFrame, y_test: pd.Series,\n",
        "                                     max_samples: int = 5, num_features: int = 10):\n",
        "        \"\"\"\n",
        "        使用LIME分析错误预测样本\n",
        "\n",
        "        Args:\n",
        "            X_test: 测试特征\n",
        "            y_test: 测试标签\n",
        "            max_samples: 分析的最大样本数\n",
        "            num_features: LIME解释中显示的特征数量\n",
        "        \"\"\"\n",
        "        wrong_indices, y_pred = self.find_wrong_predictions(X_test, y_test)\n",
        "\n",
        "        if len(wrong_indices) == 0:\n",
        "            print(\"🎉 模型没有错误预测！\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n🔍 正在分析前 {min(max_samples, len(wrong_indices))} 个错误预测样本的LIME解释...\")\n",
        "\n",
        "        # 创建LIME解释器\n",
        "        lime_explainer = self._create_lime_explainer()\n",
        "\n",
        "        # 选择要分析的样本\n",
        "        sample_indices = wrong_indices[:max_samples] if len(wrong_indices) > max_samples else wrong_indices\n",
        "\n",
        "        # 为每个错误样本生成LIME解释\n",
        "        for idx in sample_indices:\n",
        "            print(f\"\\n--- 样本 {idx} 的LIME解释 ---\")\n",
        "            print(f\"真实标签: {y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]} ({self.class_names[y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]]})\")\n",
        "            print(f\"预测标签: {y_pred[idx]} ({self.class_names[y_pred[idx]]})\")\n",
        "\n",
        "            # 检查模型是否有predict_proba方法\n",
        "            if hasattr(self.model, 'predict_proba'):\n",
        "                prediction_fn = self.model.predict_proba\n",
        "            else:\n",
        "                # 如果没有predict_proba，创建一个包装函数\n",
        "                def prediction_fn(X):\n",
        "                    preds = self.model.predict(X)\n",
        "                    # 转换为概率格式（简单的one-hot编码）\n",
        "                    proba = np.zeros((len(preds), len(self.class_names)))\n",
        "                    for i, pred in enumerate(preds):\n",
        "                        proba[i, pred] = 1.0\n",
        "                    return proba\n",
        "\n",
        "            try:\n",
        "                # 生成LIME解释\n",
        "                exp = lime_explainer.explain_instance(\n",
        "                    X_test.iloc[idx].values,\n",
        "                    prediction_fn,\n",
        "                    num_features=min(num_features, len(X_test.columns))\n",
        "                )\n",
        "\n",
        "                # 显示解释\n",
        "                exp.show_in_notebook(show_table=True)\n",
        "\n",
        "                # 也可以保存为图片（可选）\n",
        "                # fig = exp.as_pyplot_figure()\n",
        "                # plt.title(f'LIME解释 - 样本 {idx}')\n",
        "                # plt.tight_layout()\n",
        "                # plt.show()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"生成样本 {idx} 的LIME解释时出错: {str(e)}\")\n",
        "\n",
        "    def complete_xai_analysis(self, X_test: pd.DataFrame, y_test: pd.Series,\n",
        "                            max_summary_samples: int = 1000, max_wrong_samples: int = 5,\n",
        "                            lime_features: int = 10, class_index: Optional[int] = None):\n",
        "        \"\"\"\n",
        "        完整的XAI分析流程\n",
        "\n",
        "        Args:\n",
        "            X_test: 测试特征\n",
        "            y_test: 测试标签\n",
        "            max_summary_samples: 生成总体图的最大样本数\n",
        "            max_wrong_samples: 分析错误预测的最大样本数\n",
        "            lime_features: LIME解释显示的特征数\n",
        "            class_index: 多分类时指定分析的类别\n",
        "        \"\"\"\n",
        "        print(\"🚀 开始完整的XAI可解释性分析...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. 生成SHAP总体分析图\n",
        "        print(\"📈 第一步: 生成SHAP总体分析图\")\n",
        "        self.generate_shap_summary_plots(X_test, max_summary_samples)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "        # 2. 分析错误预测 - SHAP\n",
        "        print(\"🔍 第二步: 使用SHAP分析错误预测\")\n",
        "        self.analyze_wrong_predictions_shap(X_test, y_test, max_wrong_samples, class_index)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "        # 3. 分析错误预测 - LIME\n",
        "        print(\"🔍 第三步: 使用LIME分析错误预测\")\n",
        "        self.analyze_wrong_predictions_lime(X_test, y_test, max_wrong_samples, lime_features)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"✅ XAI分析完成！\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3CJ6FWatwE_"
      },
      "source": [
        "After having a total view of the dataset, the next step will be using different ML model to build the detection system.\n",
        "Here we choose different models:\n",
        "\n",
        "1.   Decision Trees\n",
        "2.   Random Forests\n",
        "\n",
        "1.   Multilayer Perceptron\n",
        "2.   DNN\n",
        "\n",
        "1.   XGBoost\n",
        "2.   BiLSTM\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esxi-OrcttDx"
      },
      "outputs": [],
      "source": [
        "#DT\n",
        "# divide data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "## grid search\n",
        "# param_grid = {\n",
        "#     'max_depth': [3, 5, 7, None],\n",
        "#     'min_samples_split': [2, 5, 10],\n",
        "#     'min_samples_leaf': [1, 2, 4],\n",
        "#     'class_weight': [None, 'balanced']\n",
        "# }\n",
        "\n",
        "# grid_search = GridSearchCV(\n",
        "#     estimator=dt,\n",
        "#     param_grid=param_grid,\n",
        "#     scoring='roc_auc',\n",
        "#     cv=5,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# best_dt = grid_search.best_estimator_\n",
        "# print(f\"best paras：{grid_search.best_params_}\")\n",
        "# print(f\"best AUC：{grid_search.best_score_:.4f}\")\n",
        "\n",
        "best_dt=dt.fit(X_train, y_train)\n",
        "\n",
        "# model eval\n",
        "def evaluate_model(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    y_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    print(f\"accuracy：{accuracy_score(y, y_pred):.4f}\")\n",
        "    print(f\"precision：{precision_score(y, y_pred):.4f}\")\n",
        "    print(f\"recall：{recall_score(y, y_pred):.4f}\")\n",
        "    print(f\"F1：{f1_score(y, y_pred):.4f}\")\n",
        "    print(f\"AUC：{roc_auc_score(y, y_proba):.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot()\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\ntrain：\")\n",
        "evaluate_model(best_dt, X_train, y_train)\n",
        "\n",
        "print(\"\\ntest：\")\n",
        "evaluate_model(best_dt, X_test, y_test)\n",
        "\n",
        "# feature importance\n",
        "feature_importance = pd.Series(best_dt.feature_importances_, index=features)\n",
        "feature_importance.sort_values().plot(kind='barh')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(20, 10))\n",
        "from sklearn.tree import plot_tree\n",
        "plot_tree(\n",
        "    best_dt,\n",
        "    feature_names=features,\n",
        "    class_names=['Class 0', 'Class 1'],\n",
        "    filled=True,\n",
        "    proportion=True,\n",
        "    max_depth=3\n",
        ")\n",
        "plt.title('Decision Tree Structure')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnutOTxm59kj"
      },
      "outputs": [],
      "source": [
        "# 使用示例\n",
        "# 1. 创建分析器\n",
        "analyzer = XAIAnalyzer(best_dt, X_train, class_names=le.classes_.tolist())\n",
        "\n",
        "# 2. 执行完整分析（推荐）\n",
        "analyzer.complete_xai_analysis(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crUjy8ERKMEw"
      },
      "outputs": [],
      "source": [
        "#RF\n",
        "\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# train\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# prediction\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "#feature importance\n",
        "feature_importances = pd.Series(\n",
        "    rf_classifier.feature_importances_,  # or rf_regressor\n",
        "    index=X.columns\n",
        ")\n",
        "feature_importances = feature_importances.sort_values(ascending=False)\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importances.plot(kind='bar')\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5K8YlhlTJ1N"
      },
      "outputs": [],
      "source": [
        "# 1. 创建分析器\n",
        "analyzer = XAIAnalyzer(rf_classifier, X_train, class_names=le.classes_.tolist())\n",
        "\n",
        "# 2. 执行完整分析（推荐）\n",
        "analyzer.complete_xai_analysis(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AileZ3fNNCQc"
      },
      "outputs": [],
      "source": [
        "#MLP\n",
        "mlp_clf = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1\n",
        ")\n",
        "\n",
        "# train\n",
        "mlp_clf.fit(X_train, y_train)\n",
        "# mlp_reg.fit(X_train, y_train)\n",
        "\n",
        "# classification report\n",
        "y_pred = mlp_clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlM-U6woaJG-"
      },
      "outputs": [],
      "source": [
        "# 1. 创建分析器\n",
        "analyzer = XAIAnalyzer(mlp_clf, X_train, class_names=le.classes_.tolist())\n",
        "\n",
        "# 2. 执行完整分析（推荐）\n",
        "analyzer.complete_xai_analysis(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AprPkwnQcvge"
      },
      "outputs": [],
      "source": [
        "#DNN\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # 2 hidded layer\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # 3 hidden layer\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # outlayer\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# train\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=512,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curves')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curves')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-m8oAq03IgQ"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import warnings\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def predict_proba(model, X):\n",
        "    preds = model.predict(X)\n",
        "    # 对于二分类，输出两列，每列是一个类别的概率\n",
        "    proba = np.hstack([1 - preds, preds])  # 1 - preds 是第一个类别的概率，preds 是第二个类别的概率\n",
        "    return proba\n",
        "\n",
        "# 然后将模型的 predict_proba 方法设置为 predict_proba\n",
        "model.predict_proba = lambda X: predict_proba(model, X)\n",
        "\n",
        "def shap_plots(model, X_train, X_test, feature_names=None):\n",
        "    \"\"\"\n",
        "    生成模型所有特征的整体SHAP条形图\n",
        "\n",
        "    参数:\n",
        "    model: 训练好的Keras模型\n",
        "    X_train: 训练集特征数据\n",
        "    X_test: 测试集特征数据\n",
        "    feature_names: 特征名称列表，如果为None则自动生成\n",
        "    \"\"\"\n",
        "    print(\"正在计算SHAP值并生成条形图...\")\n",
        "    print(f\"特征数量: {X_train.shape[1]}\")\n",
        "\n",
        "    # 创建特征名称\n",
        "    if feature_names is None:\n",
        "        feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
        "\n",
        "    # 创建SHAP解释器 - 使用DeepExplainer对深度学习模型\n",
        "    explainer = shap.Explainer(model, X_train[:100])  # 使用前100个样本作为背景数据\n",
        "\n",
        "    # 计算测试集的SHAP值\n",
        "    shap_values = explainer.shap_values(X_test[:1000])  # 计算前500个测试样本的SHAP值\n",
        "\n",
        "    print(f\"SHAP值形状: {shap_values.shape}\")\n",
        "\n",
        "    # 生成条形图\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    shap.summary_plot(shap_values, X_test[:1000],\n",
        "                     feature_names=feature_names,\n",
        "                     plot_type=\"bar\",\n",
        "                     show=False,\n",
        "                     max_display=min(20, len(feature_names)))  # 最多显示20个特征\n",
        "    plt.title(\"SHAP Feature Importance - Bar Plot\", fontsize=16, fontweight='bold')\n",
        "    plt.xlabel(\"Mean |SHAP Value|\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"条形图生成完成！\")\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    shap.summary_plot(shap_values, X_test[:1000], feature_names=feature_names,show=False)\n",
        "    plt.title(\"SHAP Feature Importance - Beeswarm Plot\", fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"蜂群图生成完成！\")\n",
        "\n",
        "\n",
        "\n",
        "def shap_waterfallplot_wrong_predictions(model, X_train, X_test, y_test, feature_names=None):\n",
        "    \"\"\"\n",
        "    生成五个错误预测样本的SHAP瀑布图\n",
        "\n",
        "    参数:\n",
        "    model: 训练好的Keras模型\n",
        "    X_train: 训练集特征数据\n",
        "    X_test: 测试集特征数据\n",
        "    y_test: 测试集真实标签\n",
        "    feature_names: 特征名称列表，如果为None则自动生成\n",
        "    \"\"\"\n",
        "    print(\"正在识别错误预测并生成瀑布图...\")\n",
        "    print(f\"特征数量: {X_train.shape[1]}\")\n",
        "\n",
        "    # 创建特征名称\n",
        "    if feature_names is None:\n",
        "        feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
        "\n",
        "    # 获取模型预测 - 添加verbose=0减少输出\n",
        "    y_pred_proba = model.predict(X_test, verbose=0)\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "    # 确保y_test是numpy数组\n",
        "    y_test_array = np.array(y_test).flatten()\n",
        "\n",
        "    # 找到错误预测的索引\n",
        "    wrong_indices = np.where(y_pred != y_test_array)[0]\n",
        "\n",
        "    if len(wrong_indices) == 0:\n",
        "        print(\"未发现错误预测！模型在测试集上表现完美。\")\n",
        "        return\n",
        "\n",
        "    # 选择前5个错误预测（如果不足5个则全部选择）\n",
        "    num_plots = min(5, len(wrong_indices))\n",
        "    selected_indices = wrong_indices[:num_plots]\n",
        "\n",
        "    print(f\"发现 {len(wrong_indices)} 个错误预测，将展示前 {num_plots} 个的瀑布图\")\n",
        "\n",
        "    # 创建SHAP解释器\n",
        "    explainer = shap.DeepExplainer(model, X_train[:100])\n",
        "\n",
        "    explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
        "                  X_train,\n",
        "                  feature_names=feature_names,\n",
        "                  class_names=le.classes_.tolist(),\n",
        "                  mode='classification'\n",
        "                  )\n",
        "\n",
        "    # 为每个错误预测生成瀑布图\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        print(f\"正在生成第 {i+1} 个错误预测的瀑布图...\")\n",
        "\n",
        "        try:\n",
        "            # 计算单个样本的SHAP值\n",
        "            sample_shap_values = explainer.shap_values(X_test[idx:idx+1])\n",
        "\n",
        "            # 确保数据格式正确\n",
        "            shap_vals = np.array(sample_shap_values).flatten()\n",
        "            feature_vals = np.array(X_test[idx]).flatten()\n",
        "\n",
        "            print(f\"SHAP值数量: {len(shap_vals)}, 特征值数量: {len(feature_vals)}\")\n",
        "\n",
        "            # 获取预测信息\n",
        "            true_label = int(y_test_array[idx])\n",
        "            pred_proba = float(y_pred_proba[idx][0])\n",
        "            pred_label = int(y_pred[idx])\n",
        "\n",
        "            # 处理base_values\n",
        "            base_val = float(explainer.expected_value)\n",
        "\n",
        "            # 生成瀑布图\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            # 创建Explanation对象用于瀑布图\n",
        "            explanation = shap.Explanation(\n",
        "                values=shap_vals,\n",
        "                base_values=base_val,\n",
        "                data=feature_vals,\n",
        "                feature_names=feature_names\n",
        "            )\n",
        "\n",
        "            shap.waterfall_plot(explanation, show=False, max_display=10)\n",
        "            plt.title(f\"index #{idx} - true label: {true_label}, prediction: {pred_label}, prob: {pred_proba:.3f}\",\n",
        "                     fontsize=14, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            exp = explainer_lime.explain_instance(X_test[idx], model.predict_proba, num_features=10)\n",
        "            exp.show_in_notebook(show_table=True)\n",
        "        except Exception as e:\n",
        "            print(f\"生成第 {i+1} 个瀑布图时出错: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(\"瀑布图生成完成！\")\n",
        "\n",
        "\n",
        "# 使用示例函数\n",
        "def run_shap_analysis(model, X_train, X_test, y_test, feature_names=None):\n",
        "    \"\"\"\n",
        "    运行完整的SHAP分析\n",
        "\n",
        "    参数:\n",
        "    model: 训练好的Keras模型\n",
        "    X_train: 训练集特征数据\n",
        "    X_test: 测试集特征数据\n",
        "    y_test: 测试集真实标签\n",
        "    feature_names: 特征名称列表，如果为None则自动生成\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"开始SHAP可解释性分析\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. 生成条形图\n",
        "    print(\"\\n1. 生成特征重要性...\")\n",
        "    shap_plots(model, X_train, X_test, feature_names)\n",
        "\n",
        "    # 3. 生成错误预测的瀑布图\n",
        "    print(\"\\n3. 生成错误预测的瀑布图...\")\n",
        "    shap_waterfallplot_wrong_predictions(model, X_train, X_test, y_test, feature_names)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SHAP分析完成！\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# 使用方法:\n",
        "# 确保已安装shap: pip install shap\n",
        "#\n",
        "# 如果您有特征名称列表：\n",
        "feature_names = X.columns.to_list()  # 您的实际特征名称\n",
        "#\n",
        "# 调用单个函数:\n",
        "# shap_barplots(model, X_train, X_test, feature_names)\n",
        "# shap_beeswarmplot(model, X_train, X_test, feature_names)\n",
        "# shap_waterfallplot_wrong_predictions(model, X_train, X_test, y_test, feature_names)\n",
        "#\n",
        "# 或运行完整分析:\n",
        "run_shap_analysis(model, X_train, X_test, y_test, feature_names)\n",
        "#\n",
        "# 如果没有特征名称，会自动生成Feature_0, Feature_1等名称"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba(model, X):\n",
        "    preds = model.predict(X)\n",
        "    # 对于二分类，输出两列，每列是一个类别的概率\n",
        "    proba = np.hstack([1 - preds, preds])  # 1 - preds 是第一个类别的概率，preds 是第二个类别的概率\n",
        "    return proba\n",
        "\n",
        "# 然后将模型的 predict_proba 方法设置为 predict_proba\n",
        "model.predict_proba = lambda X: predict_proba(model, X)\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "                  X_train,\n",
        "                  feature_names=feature_names,\n",
        "                  class_names=le.classes_.tolist(),\n",
        "                  mode='classification'\n",
        "                  )\n",
        "exp = explainer.explain_instance(X_test[7233], model.predict_proba, num_features=)\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "CTc1yID4_lzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDiGpTcmev1i"
      },
      "outputs": [],
      "source": [
        "#XGBoost\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "xgb_clf = XGBClassifier(\n",
        "    # objective='multi:softmax',\n",
        "    objective='binary:logistic',\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    eval_metric='logloss',\n",
        "    early_stopping_rounds=10,\n",
        "    scale_pos_weight=1\n",
        ")\n",
        "\n",
        "xgb_clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
        "y_proba = xgb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# evaluation\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8by1NN3Il0YG"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Solution 1: Use TreeExplainer instead of KernelExplainer (Recommended)\n",
        "def shap_plots_fixed(model, X_train, X_test):\n",
        "    \"\"\"\n",
        "    Fixed version using TreeExplainer which is more efficient and compatible\n",
        "    \"\"\"\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X_test[:1000])\n",
        "\n",
        "    # For binary classification, shap_values might be a list or 2D array\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values = shap_values[1]  # Use positive class\n",
        "    elif len(shap_values.shape) == 3:\n",
        "        shap_values = shap_values[:, :, 1]  # Use positive class\n",
        "\n",
        "    shap.summary_plot(shap_values, X_test[:1000], plot_type=\"bar\")\n",
        "    shap.summary_plot(shap_values, X_test[:1000])\n",
        "\n",
        "\n",
        "def lime_plot(model, X_train, X_test,Y_train, row_index,class_names):\n",
        "    import lime\n",
        "    import lime.lime_tabular\n",
        "    # Create a Lime explainer object\n",
        "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    X_train.values,\n",
        "    training_labels=Y_train,\n",
        "    feature_names=X_train.columns.tolist(),\n",
        "    class_names=class_names,\n",
        "    mode='classification'\n",
        "    )\n",
        "\n",
        "    exp = explainer.explain_instance(X_test.iloc[row_index], model.predict_proba, num_features=len(X_train.columns))\n",
        "    exp.show_in_notebook(show_table=True)\n",
        "\n",
        "def shap_waterfallplot_wrong_predictions_fixed(model, X_train, X_test, y_test,Y_train,class_names):\n",
        "    \"\"\"\n",
        "    Fixed version for waterfall plots\n",
        "    \"\"\"\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Find wrong predictions\n",
        "    wrong_indices = np.where(y_pred != y_test)[0]\n",
        "\n",
        "    if len(wrong_indices) == 0:\n",
        "        print(\"No wrong predictions found!\")\n",
        "        return\n",
        "\n",
        "    # Show waterfall plot for first wrong prediction\n",
        "    num_plots = min(5, len(wrong_indices))\n",
        "    selected_indices = wrong_indices[:num_plots]\n",
        "\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        print(f\"Generating waterfall plot for index {idx}...\")\n",
        "        # Handle different shap_values formats\n",
        "        if isinstance(shap_values, list):\n",
        "            shap_vals = shap_values[1][idx]  # Use positive class\n",
        "        elif len(shap_values.shape) == 3:\n",
        "            shap_vals = shap_values[idx, :, 1]  # Use positive class\n",
        "        else:\n",
        "            shap_vals = shap_values[idx]\n",
        "\n",
        "    # Create explanation object for waterfall plot\n",
        "        explanation = shap.Explanation(\n",
        "            values=shap_vals,\n",
        "            base_values=explainer.expected_value if not isinstance(explainer.expected_value, list) else explainer.expected_value[1],\n",
        "            data=X_test.iloc[idx] if hasattr(X_test, 'iloc') else X_test[idx]\n",
        "        )\n",
        "\n",
        "        shap.waterfall_plot(explanation)\n",
        "        lime_plot(model, X_train, X_test,Y_train, idx,class_names)\n",
        "\n",
        "\n",
        "# Updated function calls - replace your original functions with these:\n",
        "def run_shap_analysis(model, X_train, X_test, y_test,Y_train,class_names):\n",
        "    \"\"\"\n",
        "    Run all SHAP analyses with fixed functions\n",
        "    \"\"\"\n",
        "    print(\"Creating bar plots...\")\n",
        "    shap_plots_fixed(model, X_train, X_test)\n",
        "\n",
        "    print(\"Creating waterfall plots for wrong predictions...\")\n",
        "    shap_waterfallplot_wrong_predictions_fixed(model, X_train, X_test, y_test,Y_train,class_names)\n",
        "\n",
        "# Usage example:\n",
        "class_names = le.classes_.tolist()\n",
        "run_shap_analysis(xgb_clf, X_train, X_test, y_test,y_train,class_names)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tWB8tx2CZnexlY3B-uhwMZP-y89YESbq",
      "authorship_tag": "ABX9TyN7hekdUdgsc9UHHGu7qAK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}